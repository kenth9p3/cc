{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "from tqdm import tqdm, trange\n",
    "from collections import Counter\n",
    "from itertools import chain \n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, multilabel_confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATASET USING SKLEARN TRAIN_TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = {\n",
    "    'train': pd.read_csv('../data/revised-dataset/train_revised.csv').reset_index(drop=True),  # \"\"\" encoding='cp1252' \"\"\" insert between train_data.csv and .reset index as parameter\n",
    "    'test': pd.read_csv('../data/revised-dataset/test_revised.csv').reset_index(drop=True),\n",
    "    'val': pd.read_csv('../data/revised-dataset/eval_revised.csv').reset_index(drop=True)\n",
    "}\n",
    "\n",
    "LABELS = ['Age', 'Gender', 'Physical', 'Race', 'Religion', 'Others']\n",
    "\n",
    "class MLTHSDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, labels, max_token_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.max_token_len = max_token_len\n",
    "        self.encoded_dataset = self.encode_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_dataset[index]\n",
    "    \n",
    "\n",
    "    def encode_dataset(self):\n",
    "        encoded_dataset = []\n",
    "        for index, data in tqdm(self.data.iterrows()):\n",
    "            encoded_data = self.encode_data(data)\n",
    "            encoded_dataset.append(encoded_data)\n",
    "        return encoded_dataset\n",
    "\n",
    "    def encode_data(self, data):\n",
    "        text = data[\"Text\"]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        labels = [data[label] for label in self.labels]\n",
    "\n",
    "        representation = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(labels)\n",
    "        }\n",
    "        return representation\n",
    "\n",
    "class MLTHSDataLoader:\n",
    "    def __init__(self, dataset, labels, tokenizer, batch_size=8):\n",
    "        self.train_dataset = MLTHSDataset(dataset['train'], tokenizer, labels)\n",
    "        self.val_dataset = MLTHSDataset(dataset['val'], tokenizer, labels)\n",
    "        self.test_dataset = MLTHSDataset(dataset['test'], tokenizer, labels)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, get_cosine_schedule_with_warmup\n",
    "\n",
    "xlnet_model_name = \"xlnet-base-uncased\"\n",
    "\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model_name, do_lower_case=True)\n",
    "\n",
    "LABELS = ['Age', 'Gender', 'Physical', 'Race', 'Religion', 'Others']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_column):\n",
    "\n",
    "    text_column = text_column.apply(lambda x: re.sub(r'[A-Z]', lambda y: y.group(0).lower(), x))\n",
    "    # Removal of unimportant links\n",
    "    text_column = text_column.apply(lambda x: re.sub(r'http[s]?://\\S+', '', x))\n",
    "\n",
    "    # emoji \n",
    "    text_column = text_column.apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
    "\n",
    "    # username\n",
    "    text_column = text_column.apply(lambda x: re.sub(r'@\\w+', '', x))\n",
    "\n",
    "    # punctuations\n",
    "    text_column = text_column.apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "\n",
    "    # hashtag\n",
    "    text_column = text_column.apply(lambda x: re.sub(r'#', '', x))\n",
    "    \n",
    "    return text_column"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
